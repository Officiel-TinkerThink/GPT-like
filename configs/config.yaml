GPT_CONFIG_124M:
  vocab_size: 50257
  context_length: 256
  emb_dim: 768
  n_heads: 12
  n_layers: 12
  drop_rate: [0.1, 0.1, 0.1]
  qkv_bias: False

GPT_CONFIG_MEDIUM:
  vocab_size: 50257
  context_length: 1024
  emb_dim: 1024
  n_heads: 16
  n_layers: 24
  drop_rate: [0.1, 0.1, 0.1]
  qkv_bias: False

GPT_CONFIG_LARGE:
  vocab_size: 50257
  context_length: 1024
  emb_dim: 1280
  n_heads: 20
  n_layers: 36
  drop_rate: [0.1, 0.1, 0.1]
  qkv_bias: False

GPT_CONFIG_XL:
  vocab_size: 50257
  context_length: 1024
  emb_dim: 1600
  n_heads: 25
  n_layers: 48
  drop_rate: [0.1, 0.1, 0.1]
  qkv_bias: False

hyperparams:
  train_ratio: 0.9
  num_epochs: 1
  eval_freq: 5
  eval_iter: 5
  start_context: "Every effort moves you"

data:
  train_ratio: 0.9
  textfile_path: "./the-verdict.txt"
  tokenizer: 'gpt2'
  plot_dir: './plot'


